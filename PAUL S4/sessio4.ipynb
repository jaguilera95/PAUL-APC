{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sessió 4: PyTorch & Búsqueda aproximada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOM**: ####\n",
    "\n",
    "## **NIU**: ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "En aquesta sessió continuarem utilitzant pytorch per definir i evaluar diferents models amb xarxes neuronals.\n",
    "\n",
    "* Durant la classe, repasarem el codi aqui mostrat i veurem què podem provar.\n",
    "* **A casa**\n",
    " * Apartat A. Búsqueda Brute Force **(2pts)**\n",
    " * Apartat B. Búsqueda Aproximada **(3pts)**\n",
    " * Apartat C. Net Encoding. Búsqueda Brute Force **(2pts)**\n",
    " * Apartat D. Net Encoding. Búsqueda Aproximada **(3pts)** \n",
    " \n",
    "\n",
    "Treballarem sobre la base de dades [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist). És similar a la base de dades de MNIST, un dataset clàssic en la visió per computador. Són imatges de 28x28 pixels i en escala de grisos. El original disposa de 60.000 imatges de entrenament i 10.000 de test. Està anotat en 10 categories diferents:\n",
    "\n",
    "|Id|Nom|\n",
    "|:-:|:--|\n",
    "|0|T-shirt/top\n",
    "|1|Trouser |\n",
    "|2|Pullover|\n",
    "|3|Dress|\n",
    "|4|Coat|\n",
    "|5|Sandal|\n",
    "|6|Shirt|\n",
    "|7|Sneaker|\n",
    "|8|Bag |\n",
    "|9|Ankle boot|\n",
    "\n",
    "<img src=\"https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Aqui definim els principals parametres que poden fer variar el metode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quick_experiment = True  # reduce the number of training and testing samples for a quick check\n",
    "epochs_in_quick = 2      # number of epochs in the fast experiment\n",
    "n_train_in_quick = 60000  # number of training images in the fast experiment\n",
    "n_test_in_quick = 1000    # number of testing images in the fast experiment\n",
    "\n",
    "epochs = 5             # number of epochs to train (default: 14)\n",
    "learning_rate = 0.001   # learning rate (default: 0.001-[0.01]-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si fem un experiment rapid, quin % del total estem agafant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING USED:  100.0%\n",
      "TESTING  USED:  10.0%\n"
     ]
    }
   ],
   "source": [
    "total_training_data = 60000 # no tocar. només serveix per visualitzar\n",
    "total_testing_data = 10000  # no tocar. només serveix per visualitzar\n",
    "\n",
    "if quick_experiment:\n",
    "    total_training_data = 100.*n_train_in_quick/total_training_data\n",
    "    total_testing_data = 100.*n_test_in_quick/total_testing_data\n",
    "else:\n",
    "    total_training_data = 100\n",
    "    total_testing_data = 100\n",
    "    \n",
    "print(\"TRAINING USED:  {:.1f}%\\nTESTING  USED:  {:.1f}%\".format(total_training_data, total_testing_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "En principi els següents parametres son força estandard i no caldria tocar-los gaire si no sabeu el que volen dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CUDA: False\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100        # number of samples during training\n",
    "test_batch_size = 1000  # number of samples for test \n",
    "\n",
    "no_cuda = True          # disables CUDA training\n",
    "dry_run = False         # quickly check a single pass\n",
    "seed = 1                # random seed (default: 1)\n",
    "log_interval = 50       # how many batches to wait before logging training status\n",
    "save_model = False      # For Saving the current Model\n",
    "\n",
    "\n",
    "# Check if cuda is available\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "print(f\"USING CUDA: {use_cuda}\")\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# define the device where to compute (cpu or gpu)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=64 * 6 * 6, out_features=600)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class FashionCNN_feature_extraction(FashionCNN):\n",
    "    def __init__(self):\n",
    "        super(FashionCNN_feature_extraction, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.fc2(out)\n",
    "#        out = self.fc3(out)\n",
    "#        out = self.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Funcions auxiliars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(y_pred, y_real):\n",
    "    # mostra la matriu de confusió\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "    plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(cm, annot = True, fmt = 'g')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def calcular_parametres_del_model(current_model):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"# trainable parameters: {:,}\".format(pytorch_total_params))\n",
    "    return pytorch_total_params\n",
    "\n",
    "def calculate_parameters_and_flops(current_model):\n",
    "    from thop import profile\n",
    "    test_input = torch.randn(1, 1, 28, 28)\n",
    "    macs, params = profile(current_model, inputs=(test_input,))  # multiply accumulate operation (GFLOPS = 2 * GMACS)\n",
    "    # normalment, en gpus i exemples reals, es parlaria minim de Gigaflops.\n",
    "    # print(\"%s | %.2f Params(M) | %.3f FLOPs(G)\" % (current_model._get_name(), params / (1000 ** 2), macs / (1000 ** 3)))\n",
    "    print(\"%s | %.2f Params(M) | %.3f FLOPs(M)\" % (current_model._get_name(), params / (1000 ** 2), macs / (1000 ** 2)))\n",
    "    return macs, params\n",
    "    \n",
    "def mostra_estructura_model_torchviz(current_model):\n",
    "    from torchviz import make_dot\n",
    "    test_input = torch.randn(1, 1, 28, 28)\n",
    "    return make_dot(current_model(test_input), params=dict(current_model.named_parameters()))\n",
    "    \n",
    "    \n",
    "def mostra_estructura_model_hiddenlayer(current_model):\n",
    "    import hiddenlayer as hl\n",
    "    test_input = torch.randn(1, 1, 28, 28)\n",
    "    hl_graph = hl.build_graph(current_model, test_input)\n",
    "    return hl_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    t = tqdm.tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    t.set_description('Train')\n",
    "    for batch_idx, (data, target) in t:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        t.set_postfix(loss=loss.item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    totals = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t = tqdm.tqdm(test_loader, total=len(test_loader))\n",
    "        t.set_description('Test ')\n",
    "        for data, target in t:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.shape[0]  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            totals += len(target)\n",
    "            t.set_postfix(loss=test_loss/totals, accuracy=100.*correct/totals)\n",
    "            all_preds.extend(np.asarray(pred))\n",
    "            all_targets.extend(np.asarray(target))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    visualize_confusion_matrix(all_preds, all_targets)    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(model, device, loss, optimizer, train_loader, test_loader, name='', save_model=False):\n",
    "    init_time = time.time()\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracies_test = []\n",
    "    print('--'*50)\n",
    "    print('STARTING EXPERIMENT {}'.format(name))\n",
    "    print('--'*50)\n",
    "    \n",
    "    model.to(device)\n",
    "    print(\"CHECKING INITIAL TEST LOSS (with random weights..)\")\n",
    "    loss_test_epoch, accuracy_epoch = test(model, device, test_loader, loss)\n",
    "    losses_test.append(loss_test_epoch)\n",
    "    accuracies_test.append(accuracy_epoch)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print (\"EPOCH {}\".format(epoch))\n",
    "        sys.stdout.flush()\n",
    "        loss_train_epoch = train(model, device, train_loader, optimizer, loss)\n",
    "        loss_test_epoch, accuracy_epoch = test(model, device, test_loader, loss)\n",
    "\n",
    "        losses_train.extend(loss_train_epoch)\n",
    "        losses_test.append(loss_test_epoch)\n",
    "        accuracies_test.append(accuracy_epoch)\n",
    "\n",
    "    plt.plot(range(len(losses_train)), \n",
    "             losses_train, label=\"Training Loss\")\n",
    "    plt.plot(range(0, len(losses_train)+1, int(len(losses_train)/(len(losses_test)-1))), \n",
    "             losses_test, label=\"Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    elapsed = time.time()-init_time\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"fashion_mnist_cnn.pt\")\n",
    "\n",
    "    print (\"ELAPSED TIME: {:.1f}s\".format(elapsed))\n",
    "\n",
    "    return losses_train, losses_test, accuracies_test, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar les dades d'entrenament\n",
    "Aquí, mantindrem el codi de la sessió anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26422272it [00:01, 13913787.75it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 99360.42it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4422656it [00:00, 5928354.58it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6144it [00:00, 2052062.73it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "dataset_train = torchvision.datasets.FashionMNIST('../data', train=True, download=True, transform=transform)\n",
    "dataset_val = torchvision.datasets.FashionMNIST('../data', train=False, transform=transform)\n",
    "\n",
    "if quick_experiment:\n",
    "    epochs = epochs_in_quick\n",
    "    n_not_used_for_train_in_quick = len(dataset_train) - n_train_in_quick\n",
    "    dataset_train, dataset_train_not_used = torch.utils.data.random_split(dataset_train,\n",
    "                                                                          [n_train_in_quick,\n",
    "                                                                           n_not_used_for_train_in_quick])\n",
    "\n",
    "    n_not_used_for_test_in_quick = len(dataset_val) - n_test_in_quick\n",
    "    dataset_val, dataset_val_not_used = torch.utils.data.random_split(dataset_val,\n",
    "                                                                          [n_test_in_quick,\n",
    "                                                                           n_not_used_for_test_in_quick])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_val, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pel que fa al primer apartat de les knn, utilitzarem les imatges en raw, es a dir, el valor dels seus pixels. A continuació les obtindrem i les posarem en forma de matriu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-e17cf5af266e>:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_data = np.asarray(train_data, dtype=np.float) / 255.\n",
      "<ipython-input-11-e17cf5af266e>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_data = np.asarray(test_data, dtype=np.float) / 255.\n"
     ]
    }
   ],
   "source": [
    "if quick_experiment:\n",
    "    train_data = dataset_train.dataset.data[dataset_train.indices, :].reshape(-1, 28*28)\n",
    "    test_data = dataset_val.dataset.data[dataset_val.indices, :].reshape(-1, 28*28)\n",
    "\n",
    "    labels_train = dataset_train.dataset.targets[dataset_train.indices]\n",
    "    labels_test = dataset_val.dataset.targets[dataset_val.indices]\n",
    "else:\n",
    "    train_data = dataset_train.data.reshape(-1, 28*28)\n",
    "    test_data = dataset_val.data.reshape(-1, 28*28)\n",
    "\n",
    "    labels_train = dataset_train.targets\n",
    "    labels_test = dataset_val.targets\n",
    "\n",
    "train_data = np.asarray(train_data, dtype=np.float) / 255.\n",
    "test_data = np.asarray(test_data, dtype=np.float) / 255.    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "Farem servir aquest funció per a que construeixi el model, hi carregui les dades, executi la cerca dels veïns més propers (per cada exemple del test, buscar en el train). Amb aquest veïns, s'hi aplica un classificador de més votat (així també podem veure com afecta a la seva performance). Finalment, calculem quants dels veïns hem trobat amb el model són els reals, respecte a les distàncies exactes calculades amb el bruteforce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_knn(model, train_data, train_labels, test_data, test_labels, k=10, test_knn_truth=None):\n",
    "    print(\"Train: {}\".format(train_data.shape))\n",
    "    print(\"Test: {}\".format(test_data.shape))\n",
    "    print(\"knn available?: {}\".format(test_knn_truth is not None))\n",
    "    print(\"Model: {}\".format(model))\n",
    "    build_time = model.build(train_data, train_labels)\n",
    "    dist, idx, query_time = model.query(test_data, k=k)\n",
    "    predicted_label = model.classify_from_idx(idx)\n",
    "    accuracy = 100*torch.sum(predicted_label==test_labels)/float(len(test_labels))\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "    if test_knn_truth is None:\n",
    "        recall_at_k = 100\n",
    "    else:\n",
    "        recall_at_k = [len(np.intersect1d(idx[i,:], test_knn_truth[i,:], assume_unique=True)) \n",
    "                        for i in range(idx.shape[0])]\n",
    "        recall_at_k = 100*np.mean(recall_at_k) / idx.shape[1]\n",
    "\n",
    "    print(\"Recall@10: {:.2f}%\".format(recall_at_k))\n",
    "\n",
    "    return {\"class\": str(type(model)), \n",
    "            \"parameters\": model.__str__(),\n",
    "            \"test_samples\": test_data.shape[0], \n",
    "            \"dimensions\": test_data.shape[1], \n",
    "            \"classification_accuracy\": float(accuracy),\n",
    "            \"k\": 10,\n",
    "            \"recall@k\": recall_at_k, \n",
    "            \"build_time\": build_time, \n",
    "            \"query_time\": query_time,\n",
    "            \"queries/s\": 1.0 / (query_time / float(test_data.shape[0]))}, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartat A. Búsqueda Brute Force **(2pts)**\n",
    "\n",
    "En aquest primer apartat, el que hem de fer es calcular la distància \"real\" (o exacte) entre el dataset de test i el dataset de train. Per fer això, o bé ho feu amb codi, calculant les distàncies entre tots els samples, o bé fem servir la funcio una de les següents funcions de `scipy`:\n",
    "\n",
    "* [scipy.spatial.distance.cdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html)\n",
    "* [scipy.spatial.distance_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html)\n",
    "\n",
    "Un cop calculada la matriu resultant de les distàncies, troba els veïns més propers per cada sample de test, és a dir els topK més propers. (K=10).\n",
    "\n",
    "Per tal de fer-ho reutilitzable fàcilment, podem instanciar-ho dins la classe `BruteForce()`.\n",
    "\n",
    "**Guardeu** les distàncies exactes i els veïns més propers per cada exemple de test. Ho farem servir per calcular el recall obtingut de les búsquedes aproximades (és a dir, quants dels veïns més propers s'han trobat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BruteForce Class\n",
    "\n",
    "Implementació de la búsqueda basada en bruteforce, on es calculen les distancies entre tots els exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "class BruteForce():\n",
    "    def __init__(self, params={}):\n",
    "        self.params=params\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"BruteForce()\"\n",
    "        \n",
    "    def build(self, vectors, labels):\n",
    "        t1 = time.time()\n",
    "        self.dimension = vectors.shape[1]\n",
    "        self.vectors = vectors\n",
    "        self.labels = labels\n",
    "        self.unique_labels = np.unique(self.labels)\n",
    "\n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Build. Nothing to do. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        return elapsed\n",
    "    \n",
    "    def query(self, vectors, k=10):\n",
    "        t1 = time.time()\n",
    "        ############################\n",
    "        ##         TO DO          ##\n",
    "        ############################\n",
    "        dist, ind = None, None\n",
    "\n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Query Done. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        \n",
    "        return (dist, ind, elapsed)\n",
    "\n",
    "    def classify(self, vectors, k=10):\n",
    "        dist, idx, _ = self.query(vectors, k)\n",
    "        return torch.as_tensor([np.argmax(np.bincount(self.labels[idx[i, :]], minlength=len(self.unique_labels))) \n",
    "                                for i in range(vectors.shape[0])])\n",
    "\n",
    "    def classify_from_idx(self, idx):\n",
    "        return torch.as_tensor([np.argmax(np.bincount(self.labels[idx[i, :]], minlength=len(self.unique_labels))) \n",
    "                                for i in range(idx.shape[0])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilitzarem la funció `experiment_knn()` per a que ens faci totes les operacions amb el algoritme desitjat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 784)\n",
      "Test: (1000, 784)\n",
      "knn available?: False\n",
      "Model: BruteForce()\n",
      "Build. Nothing to do. Elapsed: 0.97s\n",
      "Query Done. Elapsed: 0.00s\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fea9ae2ba8b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_brute_force\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment_knn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBruteForce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mall_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-b7467ef1e015>\u001b[0m in \u001b[0;36mexperiment_knn\u001b[1;34m(model, train_data, train_labels, test_data, test_labels, k, test_knn_truth)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mbuild_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpredicted_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify_from_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: {:.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-64d2ff86d4dd>\u001b[0m in \u001b[0;36mclassify_from_idx\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify_from_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         return torch.as_tensor([np.argmax(np.bincount(self.labels[idx[i, :]], minlength=len(self.unique_labels))) \n\u001b[1;32m---> 40\u001b[1;33m                                 for i in range(idx.shape[0])])\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "info, idx_brute_force = experiment_knn(BruteForce(), train_data, labels_train, test_data, labels_test)\n",
    "all_info.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Implementa el mètode `query` de la classe BruteForce.**\n",
    "**Ha de retornar 3 variables. La variable distancies ordenada dels k veïns més propers, els index dels items del training més propers i el temps transcorregut. Les dues matrius tindràn la mida (n_items_test, k).**\n",
    "\n",
    "**Com pots veure, el mètode build, s'encarrega de agafar les dades d'entrenament i posar-ho en una estructura, un index o la classe que permet llavors fer-hi búsquedes. Retorna el temps transcorregut. En aquest cas, no és necessari fer gaire res, però pels següents mètodes veureu que si.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Executa la búsqueda amb diferents `k` i comprova l'accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartat B. Búsqueda Aproximada **(4pts)**\n",
    "En aquest apartat s'utilitzaran varies llibreria de búsqueda aproximada. Inicialment es faràn servir les de sklearn ([KdTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) i [BallTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html)), i també provarem la de [Annoy](https://github.com/spotify/annoy). Per instal·lar aquesta última es pot fer simplement amb un `pip install annoy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "class ANN_sklearn_kdtree():\n",
    "    def __init__(self, extra_param = {}):\n",
    "        self.model_name = model_name\n",
    "        self.vectors = None\n",
    "        self.labels = None\n",
    "        self.unique_labels = None\n",
    "        \n",
    "        self.extra_param = extra_param\n",
    "        self.leaf_size = self.extra_param[\"leaf_size\"] if \"leaf_size\" in self.extra_param else 40\n",
    "        self.breadth_first = self.extra_param[\"breadth_first\"] if \"breadth_first\" in self.extra_param else False\n",
    "        self.dualtree = self.extra_param[\"dualtree\"] if \"dualtree\" in self.extra_param else True\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ANN_sklearn kdtree ({})\".format(self.extra_param)\n",
    "    \n",
    "    def build(self, vectors, labels):\n",
    "        t1 = time.time()\n",
    "\n",
    "        ############################\n",
    "        ##         TO DO          ##\n",
    "        ############################\n",
    "        self.model = None\n",
    "\n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Index Built. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        return elapsed\n",
    "\n",
    "        \n",
    "    def query(self, vectors, k=10):\n",
    "        t1 = time.time()\n",
    "        ############################\n",
    "        ##         TO DO          ##\n",
    "        ############################\n",
    "        dist, ind = None, None\n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Query Done. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        return dist, ind, elapsed\n",
    "\n",
    "\n",
    "    def classify(self, vectors, k=10):\n",
    "        dist, idx, _ = self.query(vectors, k)\n",
    "        return torch.as_tensor([np.argmax(np.bincount(labels_train[idx[i, :]], minlength=10)) \n",
    "                                for i in range(vectors.shape[0])])\n",
    "\n",
    "    def classify_from_idx(self, idx):\n",
    "        return torch.as_tensor([np.argmax(np.bincount(self.labels[idx[i, :]], minlength=10)) \n",
    "                                for i in range(idx.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info.append(experiment_knn(ANN_sklearn_kdtree(extra_param={\"leaf_size\": 5}), \n",
    "                               train_data, labels_train, \n",
    "                               test_data, labels_test,\n",
    "                               test_knn_truth=idx_brute_force)[0])\n",
    "all_info.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "class ANN_sklearn_balltree():\n",
    "    def __init__(self, extra_param = {}):\n",
    "        self.vectors = None\n",
    "        self.labels = None\n",
    "        self.unique_labels = None\n",
    "        \n",
    "        self.extra_param = extra_param\n",
    "        self.leaf_size = self.extra_param[\"leaf_size\"] if \"leaf_size\" in self.extra_param else 40\n",
    "        self.breadth_first = self.extra_param[\"breadth_first\"] if \"breadth_first\" in self.extra_param else False\n",
    "        self.dualtree = self.extra_param[\"dualtree\"] if \"dualtree\" in self.extra_param else True\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ANN_sklearn_balltree ({})\".format(self.extra_param)\n",
    "    \n",
    "    def build(self, vectors, labels):\n",
    "        t1 = time.time()\n",
    "\n",
    "        ############################\n",
    "        ##         TO DO          ##\n",
    "        ############################\n",
    "        self.model = None\n",
    "        \n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Index Built. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        return elapsed\n",
    "\n",
    "        \n",
    "    def query(self, vectors, k=10):\n",
    "        t1 = time.time()\n",
    "        ############################\n",
    "        ##         TO DO          ##\n",
    "        ############################\n",
    "        dist, ind = None, None\n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Query Done. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        return dist, ind, elapsed\n",
    "\n",
    "\n",
    "    def classify(self, vectors, k=10):\n",
    "        dist, idx, _ = self.query(vectors, k)\n",
    "        return torch.as_tensor([np.argmax(np.bincount(labels_train[idx[i, :]], minlength=10)) \n",
    "                                for i in range(vectors.shape[0])])\n",
    "\n",
    "    def classify_from_idx(self, idx):\n",
    "        return torch.as_tensor([np.argmax(np.bincount(self.labels[idx[i, :]], minlength=10)) \n",
    "                                for i in range(idx.shape[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info.append(experiment_knn(ANN_sklearn('balltree', extra_param={\"leaf_size\": 5}), \n",
    "                               train_data, labels_train, \n",
    "                               test_data, labels_test,\n",
    "                               test_knn_truth=idx_brute_force)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "class ANN_annoy():\n",
    "    def __init__(self, extra_param = {}):\n",
    "        self.model = None\n",
    "        self.extra_param = extra_param\n",
    "        self.n_trees = self.extra_param[\"n_trees\"] if \"n_trees\" in self.extra_param else 10\n",
    "        self.k_search = self.extra_param[\"k_search\"] if \"k_search\" in self.extra_param else -1\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ANN Annoy ({})\".format(self.extra_param)\n",
    "    \n",
    "    def build(self, vectors, labels):\n",
    "        t1 = time.time()\n",
    "        ############################\n",
    "        ##         TO DO          ##\n",
    "        ############################\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Index Built. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        return elapsed\n",
    "\n",
    "\n",
    "        \n",
    "    def query(self, vectors, k=10):\n",
    "        t1 = time.time()\n",
    "        ############################\n",
    "        ##         TO DO          ##\n",
    "        ############################\n",
    "\n",
    "        dist, ind = None, None\n",
    "        \n",
    "        elapsed = time.time()-t1\n",
    "        print(\"Query Done. Elapsed: {:.2f}s\".format(elapsed))\n",
    "        return dist, ind, elapsed\n",
    "\n",
    "\n",
    "    def classify(self, vectors, k=10):\n",
    "        dists, idxs, _ = self.query(vectors, k)\n",
    "        return torch.as_tensor([np.argmax(np.bincount(labels_train[idxs[i]], minlength=10)) \n",
    "                                for i in range(vectors.shape[0])])\n",
    "\n",
    "    def classify_from_idx(self, idx):\n",
    "        return torch.as_tensor([np.argmax(np.bincount(self.labels[idx[i, :]], minlength=10)) \n",
    "                                for i in range(idx.shape[0])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info.append(experiment_knn(ANN_annoy(extra_param={\"n_trees\":10, \"k_search\": -1}), \n",
    "                         train_data, labels_train, test_data, labels_test,\n",
    "                         test_knn_truth=idx_brute_force)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Implementa els mètode `build` i `query` de la classe `ANN_sklearn_kdtree`. Compara el resultat amb bruteforce.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Implementa els mètode `build` i `query` de la classe `ANN_sklearn_balltree`. Compara el resultat amb bruteforce i els altres ANN.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Implementa els mètode `build` i `query` de la classe `ANN_annoy`. Compara el resultat amb bruteforce.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartat C. Net Encoding **(2pts)**\n",
    "En aquest apartat, el que farem servir és convertir les dades d'entrada (en el cas anterior el valors dels pixels de les imatges convertides en un array unidimensionals) a unes representacions corresponents a extreure els valors intermitjos al haver-hi aplicat una xarxa neuronal. \n",
    "\n",
    "Farem servir les que vàrem aprendre sobre el mateix dataset la sessió anterior, però la gràcia és que el mètode es podria aplicar (segurament amb pitjors resultats) per una xarxa genèrica apresa amb altres dades (pero del mateix àmbit). Com veureu, les característiques del apartat anterior tenen una dimensionalitat de 784, és a dir els 28*28 pixels. \n",
    "\n",
    "Són dades prou fàcils, sense soroll de fons, de mida 'tractable'.. Si ho apliquèssim a imatges reals de color.. la mida seria molt més gran (per exemple, de 300\\*300\\*3 obtindriem uns exemples de dimensionalitat 270.000, o de 640\\*480\\*3 tindriem 921.600). El fet de codificar les imatges amb característiques extretes de xarxes neuronal permet reduir moltissim la dimensionalitat, però a més, ho aconseguim capturant informació d'alt nivell en la representació.\n",
    "\n",
    "Si no teniu un model après del exercici anterior, podem executar el següent model i n'obtindrem un que ja ens servirà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionCNN()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_train, loss_test, acc_test, elapsed = experiment(model, \n",
    "                                                      device, \n",
    "                                                      loss, \n",
    "                                                      optimizer, \n",
    "                                                      train_loader, \n",
    "                                                      test_loader, \n",
    "                                                      name='#', \n",
    "                                                      save_model=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi ha varies formes d'extreure informació de les capes intermitjes d'una capa:\n",
    "* aplicar [forward_hooks](https://discuss.pytorch.org/t/how-to-register-forward-hooks-for-each-module/43347/5)\n",
    "* en el forward, [retornar varies variables](https://discuss.pytorch.org/t/accessing-intermediate-layers-of-a-pretrained-network-forward/12113/4) (no només la sortida final)\n",
    "* en el forward, retornar la capa que ens interessa.\n",
    "\n",
    "Aquí farem servir la última opció, per fer-ho més fàcil. Amb el model après, carregarem els pesos en una nova definicio de xarxa que no executi tots els passos. Ho farem amb la següent funció: (aquelles capes que es diuen igual, s'hi copien els pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_extract_features(input_model):\n",
    "    output_model = FashionCNN_feature_extraction()\n",
    "    output_model.load_state_dict(model.state_dict())\n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = convert_model_to_extract_features(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, device, data_loader):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t = tqdm.tqdm(data_loader, total=len(data_loader))\n",
    "        t.set_description('Extract Features ')\n",
    "        for data, target in t:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            all_features.append(np.asarray(output))\n",
    "\n",
    "        all_features = np.vstack(all_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = extract_features(new_model, device, train_loader)\n",
    "test_data_features = extract_features(new_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_features.shape)\n",
    "print(test_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, idx_brute_force_features = experiment_knn(BruteForce(), \n",
    "                                                train_data_features, labels_train, \n",
    "                                                test_data_features, labels_test)\n",
    "all_info.append(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Aprèn un model i extreu les caracteristiques de la penúltima capa.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Modifica la dimensionalitat del model de la capa fc2 (un amb més dimensions i un altre amb menys) i reaprèn els models i compara'ls.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartat D. Net Encoding. Búsqueda BruteForce i Aproximada **(3pts)**\n",
    "\n",
    "Finalment, referem les búsquedes dels apartats A i B, però utilitzant les dades aconseguides en el apartat C. D'aquesta forma, veurem com ha influit la reducció de dimensionalitat en els mètodes.\n",
    "\n",
    "Un cop tinguem tots els resultats, el que farem és mostrar-los gràficament, on a les X hi trobem el recall aconseguit i a les Y el temps que ha trigat en fer les N queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Compara els resultats del bruteforce i els 3 ANN utilitzant les caracteristiques de la xarxa. Explica les diferències respecte utilitzant les dades originals.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Executa cerques amb varis paràmetres dels ANN.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **PREGUNTA: Mostra una gràfica mostrant les queries/s respecte el recall que aconsegueixen.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

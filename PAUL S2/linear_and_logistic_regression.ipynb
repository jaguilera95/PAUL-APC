{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Linear Regression i Logistic Regression en PyTorch\n",
    "### Sessió 2 Problemes\n",
    "\n",
    "Hem extret aquesta petita intro d'aquest blog:\n",
    "\n",
    "https://medium.com/biaslyai/pytorch-linear-and-logistic-regression-models-5c5f0da2cb9\n",
    "\n",
    "## Outline\n",
    "* Linear Regression\n",
    "* Define Model Structure\n",
    "* Loss Function (Criterion) and Optimizer\n",
    "* Model Training\n",
    "* Make Predictions\n",
    "* Logistic Regression\n",
    "\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/640/1*QiU6DcP_r9qWLznMw0-M_Q.png\">\n",
    "\n",
    "Linear regression is a method commonly used for predictive statistical analysis. The objective of the regression task is to explain and make adequate predictions based on the linear relation with an independent variable. In other words, we would like to find the best-fitting line through the independent variables present in our data. The simplest form of the linear regression equation is defined by:\n",
    "\n",
    "$Ŷ = bX + a + e$\n",
    "\n",
    "where,\n",
    "\n",
    "$Ŷ$ = Predicted value of $Y$\n",
    "\n",
    "$X$ = Independent variable\n",
    "\n",
    "$b$ = Slope coefficient based on best-fitting line\n",
    "\n",
    "$a$ = Intercept\n",
    "\n",
    "$e$ = Error term\n",
    "\n",
    "\n",
    "## Define Model Structure\n",
    "Now, let’s observe how we set up the base structure of this model in PyTorch. We first import library functions and define some toy data points. Here, x_data is the independent variable, and y_data is the target variable to be learned by the model. The model learns the best-fitting regression line through the slope coefficient and Y-intercept. Then, it can make new predictions with newly introduced x_data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_data = torch.Tensor([[0.1], [0.4], [0.6], [0.7], [0.3], [.15], [0.5], [.45]])\n",
    "y_data = torch.Tensor([[2.6], [3.5], [4.0], [4.5], [3.1], [2.8], [4.1], [3.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set up the model class, we need to initialize the model type and declare the forward pass. Since our model takes one independent variable and makes one prediction for the Ŷ variable at a time, we initialize our model with this linear layer: torch.nn.Linear(1, 1) , where the first 1 is for the input size, and the second 1 is for the output size.\n",
    "Next is to define the forward pass function. The forward pass refers to the calculation process of the output data from the input. We simply define as below. The function takes x as its input and outputs the predicted value of Y, y_pred .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function (Criterion) and Optimizer\n",
    "After the forward pass, a loss function is calculated from the target y_data and the prediction y_pred in order to update weights for the best model selection in the further step. Setting up the loss function is a fairly simple step in PyTorch. Here, we will use the Mean Square Error (MSE), which is the most commonly used regression loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Stochastic Gradient Descent (SGD) optimizer for the update of hyperparameters. model.parameters() will provide the learnable parameters to the optimizer and lr=0.01 defines the learning rates for the parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Our model is now ready to train. We begin by setting up an epoch size. Epoch is a single pass through whole training dataset. In the example below, the epoch size is set to 20, meaning there will be 20 single passes of the training and weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.train()  # ens posem en mode 'train'\n",
    "for epoch in range(20):  # fem 20 epoques \n",
    "    optimizer.zero_grad()  # resetejem els gradients a zero\n",
    "    # Forward pass\n",
    "    y_pred = model(x_data) # fem la predicció\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_data) # mirem el error\n",
    "    # Backward pass\n",
    "    loss.backward()     # calculem els gradients\n",
    "    optimizer.step()    # actualitzem els pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After the forward pass and the loss computation is done, we do a backward pass, which refers to the process of learning and updating the weights.\n",
    "\n",
    "We first need to set our gradient to zero: optimizer.zero_grad(). This is because every time a variable is back propagated through, the gradient will be accumulated instead of being replaced. Then we run a backward pass by loss.backward() .\n",
    "\n",
    "The optimizer.step() performs a parameter update based on the current gradient.\n",
    "\n",
    "## Make Predictions\n",
    "Now that our model is trained, we can simply make a new prediction by inputting a new x value to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Y value:  tensor(4.2771)\n"
     ]
    }
   ],
   "source": [
    "new_x = torch.Tensor([[1.0]])\n",
    "y_pred = model(new_x)\n",
    "print(\"predicted Y value: \", y_pred.data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "Now that we know how to build a linear regression model, let’s look into building a logistic regression model. Just like a linear regression model, logistic regression is predictive analysis. It is used when the dependent variable Y is dichotomous (binary).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/400/1*GHVJ6jGVsxbuoJj5d3cvzg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_data_cls = torch.Tensor([[-3], [2], [0], [4], [-2], [1], [5], [1.5]])\n",
    "y_data_cls = torch.Tensor([[ 0], [1], [0], [1],  [0], [0], [1], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This can be done by using a sigmoid function which outputs values between 0 and 1. Any output >0.5 will be class 1 and class 0 otherwise. Thus, the logistic regression equation is defined by:\n",
    "\n",
    "$Ŷ = σ ( bX + a + e)$\n",
    "\n",
    "$ σ = \\frac{1}{1+e^{-z}} $ \n",
    "\n",
    "In the code, a simple modification to the linear regression model creates a logistic regression model. We can simply apply functional.sigmoid to our current linear output from the forward pass: y_pred = functional.sigmoid(self.linear(x)) . The complete model class is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model_cls = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the loss function, we use Binary Cross-Entropy (BCE), which is known as the binary logarithmic loss function. This is commonly used for logistic regression tasks since we are predicting a binary value as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion_cls = torch.nn.BCELoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_cls.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The rest of the code will be the same as the linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Y value:  tensor(0.5099)\n",
      "predicted Y value Binary:  tensor(True)\n",
      "weigths learned: \n",
      "b(slope)=Parameter containing:\n",
      "tensor([[0.8845]], requires_grad=True) \n",
      "a(intercept)=Parameter containing:\n",
      "tensor([-0.5381], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model_cls.train()  # ens posem en mode 'train'\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred_cls = model_cls(x_data_cls)\n",
    "    # Compute Loss\n",
    "    loss = criterion_cls(y_pred_cls, y_data_cls)\n",
    "    # C = 1  # Regularization factor\n",
    "    #loss += torch.sum(model_cls.linear.weight**2) * (1 / C)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "new_x_cls = torch.Tensor([[1.0]])\n",
    "y_pred_cls = torch.sigmoid(model_cls(x_data_cls))\n",
    "print(\"predicted Y value: \", y_pred_cls.data[0][0])\n",
    "print(\"predicted Y value Binary: \", y_pred_cls.data[0][0] > 0.5)\n",
    "print(f\"weigths learned: \\nb(slope)={model_cls.linear.weight} \\na(intercept)={model_cls.linear.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
